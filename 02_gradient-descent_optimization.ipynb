{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77098377",
   "metadata": {},
   "source": [
    "## 2. Optimización con descenso del gradiente [OPCIONAL]\n",
    "\n",
    "Este ejercicio es opcional, no es necesario hacerlo para conseguir el APTO en la práctica, pero podéis hacerlo si queréis practicar más sobre el método del descenso del gradiente, que es muy importante en machine learning.\n",
    "\n",
    "Vamos a resolver el mismo problema usando el descenso del gradiente en lugar de la solución analítica. El descenso del gradiente es un método de optimización iterativo que usa el operador gradiente.\n",
    "\n",
    "Como es iterativo, necesitamos decirle cuándo tiene que parar y darnos la solución. En este caso, le vamos a decir que pare y nos devuelva la solución después de un número de iteraciones que le pasaremos como parámetro. La ecuación iterativa es.\n",
    "\n",
    "$$w^{t+1} = w^t - \\eta \\cdot \\nabla f(w)$$\n",
    "\n",
    "donde f es nuestra función objetivo, y w es un vector. Para la función objetivo, vamos a usar el error RSS, que querremos minimizar.\n",
    "\n",
    "$$ RSS(w) = \\frac{1}{2}\\sum_{n=1}^{N}[y_n-f(x_n)]^2$$\n",
    "\n",
    "y cuyo gradiente es:\n",
    "\n",
    "$$\\nabla RSS(w) = X^T(Xw^t-y)$$\n",
    "\n",
    "TIP: Ten en cuenta que el RSS también lo puedes escribir como $\\frac{1}{2}\\sum_{n=1}^{N}[y_n-\\hat{y_n}]^2$ donde $\\hat{y_n} = X \\hat{w}$ en cada paso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446ad8dd",
   "metadata": {},
   "source": [
    "### 2.1 Implementar el descenso del gradiente\n",
    "\n",
    "Completa las siguientes funciones para implementar el descenso del gradiente con la función objetivo de la regresión lineal. La función necesitará:\n",
    "\n",
    "- La entrada X y salida y de la regresión\n",
    "- Un punto inicial desde el que empezar a iterar\n",
    "- El número de iteraciones \n",
    "- El learning rate\n",
    "\n",
    "La función nos devolverá un array con las w resultantes de las iteraciones y otro con el valor de la función en cada paso del algoritmo, a la que también se conoce como *loss function*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73d5785",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gradient_descent(X, y, w0, n_iter, eta): \n",
    "    # Define la función que calcule n_iter iteraciones del descenso del gradiente\n",
    "    # Inicializamos variables\n",
    "    # Si eliges otra forma de resolverlo, puedes borrar esta parte\n",
    "    loss_iter = [np.inf]\n",
    "    w_iter = [w0]\n",
    "    w = w0\n",
    "    \n",
    "    # TODO 1 Añade la columna de 1s\n",
    "    \n",
    "    \n",
    "    # TODO 2 Haz un bucle para las iteraciones \n",
    "    # TODO 3 Dentro del bucle tendrás que actualizar el error y los pesos y añadirlos a las listas\n",
    "    \n",
    "    \n",
    "    # Devuelve los resultados\n",
    "    return np.array(w_iter), np.array(loss_iter)\n",
    "\n",
    "\n",
    "eta = 0.01 \n",
    "iteraciones = 2000 \n",
    "\n",
    "np.random.seed(123)\n",
    "w0 = np.random.rand(2).reshape((2,1))\n",
    "\n",
    "y = np.array([208500, 181500, 223500, 140000, 250000]).reshape((5,1))\n",
    "X = np.array( [[  0.37020659],\n",
    "               [  -0.48234664],\n",
    "               [  0.51483616],\n",
    "               [  0.38352774],\n",
    "               [  1.29888065]])\n",
    "\n",
    "weights, loss = gradient_descent(X, y, w0, iteraciones, eta)\n",
    "\n",
    "print(weights[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6e3013",
   "metadata": {},
   "source": [
    "### 2.2 Aplicar al dataset de consumo de combustible\n",
    "\n",
    "Leemos de nuevo los datos y aplicamos la función que acabamos de programar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5b684062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(398, 1)\n",
      "(398, 1)\n"
     ]
    }
   ],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "auto_mpg = fetch_ucirepo(id=9) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = auto_mpg.data.features \n",
    "y = auto_mpg.data.targets \n",
    "  \n",
    "# convert to numpy\n",
    "X_np = X['weight'].to_numpy().reshape((X.shape[0], 1))\n",
    "y_np = y.to_numpy()\n",
    "\n",
    "print(X_np.shape)\n",
    "print(y_np.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d27c55",
   "metadata": {},
   "source": [
    "Para ayudar al algoritmo, vamos a escalar la variable X, restando la media y dividiendo entre la desviación estándar. Después aplicamos la función anterior y dibujamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5438c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Aplica el modelo y dibuja la recta junto con los datos\n",
    "\n",
    "X = auto_mpg.data.features\n",
    "y = auto_mpg.data.targets\n",
    "\n",
    "# Escalamos la variable x \n",
    "\n",
    "X_np = X['weight'].to_numpy().reshape((X.shape[0], 1))\n",
    "y_np = y.to_numpy()\n",
    "\n",
    "X_gd=(X_np - X_np.mean()) / X_np.std()\n",
    "y_gd = y_np\n",
    "\n",
    "# TODO 1 Punto inicial y learning rate\n",
    "\n",
    "\n",
    "# TODO 2 Aplicamos el algoritmo\n",
    "\n",
    "\n",
    "# Límites de los ejes\n",
    "x_max = np.max(X_gd) + 1\n",
    "x_min = np.min(X_gd) -1\n",
    "\n",
    "\n",
    "# TODO 3 Dibuja la gráfica\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "madenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
